## 一页梳理版：微调 Fine-tuning 与 PEFT 方法

### 0. 前言要点

* 微调像备考闭卷：数据准备与反复试参复杂，成本高
* 典型风险：微调后模型更“专事专用”，出现**灾难性遗忘**
* 现实含义：微调不是默认选项，需做业务收益与成本权衡

---

## 学习目标

* 掌握：微调适用场景与端到端流程
* 理解：常见微调方法原理（全参 vs 高效微调）
* 知道：如何借助阿里云产品完成微调与部署

---

## 1. 关于微调，你应该知道的

### 1.1 定义

微调是在预训练模型基础上，用**特定领域数据**继续训练，使模型在目标任务上更稳定、更贴合需求。

### 1.2 微调能实现什么

**A. 风格化（角色化）**

* 目标：让模型在特定角色与语域里稳定表现
* 示例：医疗专家式答复，短句切中要点，术语更准确

**B. 格式化（系统对接）**

* 目标：让模型输出严格符合接口规范的指令或结构化结果
* 动机：仅靠 RAG 或塞整本 API 手册存在结构噪音、上下文上限、成本与延迟问题
* 结果：微调后模型可做意图识别 → 选接口 → 输出符合 API 格式的调用指令，实现端到端自动化

### 1.3 为什么要微调

核心是**效率与成本**：

* 大模型（如 72B）准确但贵且慢
* 小模型（如 1.8B）便宜且快但可能不准
* 用任务数据微调小模型，使其在特定任务上逼近大模型表现，换取吞吐与成本优势

### 1.4 微调的关键难点

* **数据**：高质量、领域相关、可代表真实分布，获取成本高
* **参数**：学习率、训练轮次等需要试错，易过拟合或欠拟合
* 过拟合：记答案不懂本质
* 欠拟合：学不动，训练与测试都差

---

## 1.5 如何进行微调

### 1.5.1 业务决策（是否值得微调）

* 需求匹配度：任务是否复杂，是否强领域化，现有模型能否满足
* 指标设计：是否能量化收益（准确率、投诉率、处理时长等）
* 数据可用性：是否有足够标注与清洗能力
* 合规隐私：个人信息处理、法规要求、偏见风险
* 资源可行性：GPU、时间、人力与预算
* ROI：商业收益是否覆盖训练与维护成本

### 1.5.2 微调流程（端到端）

1. 数据准备：收集、清洗、脱敏
2. 模型选择：选基础模型（示例用 Qwen-7B-chat）
3. 模型微调：构造训练样本（指令与输出），运行训练
4. 模型评测：评测集对比或平台评测
5. 模型集成：接入业务流程，处理异常与权限
6. 测试优化：迭代数据与参数
7. 部署上线：生产部署
8. 监控维护：性能与健康监控
9. 持续迭代：按反馈补数据与再训练
   配套要求：关键决策文档化，便于复盘与协作

### 1.5.3 示例解释（“西红市第十实验小学”）

* 微调前：通用模型对专有信息拒答或不确定
* 微调后：模型能稳定回答班主任等专有知识点
* 选择理由：

  * 若环境无法提供检索服务
  * 或希望模型“记住核心事实”并高频低延迟调用
  * 可用微调注入领域知识与固定答复风格

---

## 1.6 微调方式：全参微调 vs 高效微调（PEFT）

### 1.6.1 “微”的含义

* 数据规模更小
* 训练时间更短
* 不一定更新全部参数

### 1.6.2 两大类

* **全参微调（Full Fine-tuning）**：更新模型所有参数

  * 优点：适配充分
  * 缺点：算力与成本高

* **高效微调（PEFT）**：只训练少量新增或特定参数

  * 目标：降低训练成本与显存门槛，保持较好效果
  * 主流：LoRA、Adapter、Prefix Tuning、Prompt Tuning

---

## 1.6.1 LoRA（低秩适应）

* 思路：不直接更新大矩阵权重，增加旁路低秩矩阵 A 与 B 学习增量
* 训练参数显著减少，效果接近全参微调，通用性强
* 参数缩减直觉：矩阵越大，节省越夸张
* 小结：综合成本与效果，LoRA 常是默认优先级更高的 PEFT 方案

---

## 1.6.2 Adapter Tuning（插入适配器层）

* 思路：在模型某些层插入 Adapter，小模块训练，原参数冻结
* 特点：仅增加少量参数（示例提到约 3.6%）
* 代价：推理时多了一层计算，会引入延迟

---

## 1.6.3 Prefix Tuning（前缀向量）

* 思路：构造任务相关虚拟 token 前缀，并在每层保留可训练前缀表示
* 优点：减轻遗忘，长序列生成上有优势
* 代价：占用上下文窗口，结构更复杂，优化难度更高

---

## 1.6.4 Prompt Tuning（提示向量）

* 思路：只在输入层加入可训练 prompt tokens，引导模型任务理解
* 优点：训练成本低、适配灵活、多任务潜力
* 局限：占用上下文，复杂生成任务未必优于 Prefix 或 LoRA
* 经验：模型越大，Prompt Tuning 越可能逼近全参效果

---

## 1.6.5 PEFT 方法小结

* Adapter：推理额外开销
* Prefix：优化难，性能与参数规模不总单调
* Prompt 与 Prefix：占用上下文长度，增加 token 成本
* LoRA：综合性价比较突出

---

## 2. 在阿里云上微调大模型（选型路径）

* 有算法背景、要控细节：**PAI**

  * DSW：Notebook 开发与教程
  * DLC：分布式训练
  * EAS：部署与在线推理
* 有既有训练部署体系：GPU 云服务器或 ACK 云原生 AI 套件
* 无算法背景、要界面化：**百炼**

  * 开箱即用调优与评测流程管理
  * 重点投入在数据集准备与效果评测
* 成本构成：推理费、训练费、部署费，随模型与规格变化

---

## 本节小结的核心结论

* 微调解决两类核心诉求：**风格稳定**与**格式对接**
* 代价来自数据、试参、训练成本与遗忘风险
* 与提示词工程、RAG 形成策略组合：

  * 提示词：低成本调行为
  * RAG：证据驱动降幻觉
  * 微调：把能力固化到模型，提升稳定性与低延迟可用性

如果你下一节要写“安全合规”，可以自然承接微调的隐私与偏见风险，并给出数据脱敏、权限控制、审计与回滚等治理措施。